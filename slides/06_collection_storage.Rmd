---
title: "Big Data Analytics"
subtitle: "Data Collection and Data Storage"
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../style/ioslides.css
    template: ../style/nologo_template.html
logo: ../img/logo.png
bibliography: ../references/bigdata.bib
---


```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```


# Updates

## Goals for today

- Get to know a simple workflow for data collection
- Understand the basics of RDBMS. Why "efficient"? Useful for which situations?
- Review: set up and run SQLite. 
- R skills: Connect SQLite with R (locally).
- Cloud computing: Know how to set up a MySQL DB on AWS RDS.
- Know the very basics of data warehouses (example: BigQuery) and data lakes (example: AWS S3)

# Data Collection and Data Storage

## Gathering and compilation of raw data

### NYC taxi data

- The raw data consists of several monthly CSV-files and can be downloaded via the [TLC's website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). 

- The following short R-script automates the downloading of all available trip-record files. *NOTE*: Downloading all files can take several hours and will occupy over 200GB!

```{r eval=FALSE}
#################################
# Fetch all TLC trip recrods
# Data source: 
# https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
# Input: Monthly csv files from urls
# Output: one large csv file
# UM, St. Gallen, January 2019
#################################

# SET UP -----------------

# load packages
library(data.table)
library(rvest)
library(httr)

# fix vars
BASE_URL <- "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2018-01.csv"
OUTPUT_PATH <- "data/tlc_trips.csv"
START_DATE <- as.Date("2009-01-01")
END_DATE <- as.Date("2018-06-01")


# BUILD URLS -----------

# parse base url
base_url <- gsub("2018-01.csv", "", BASE_URL)
# build urls
dates <- seq(from= START_DATE,
                   to = END_DATE,
                   by = "month")
year_months <- gsub("-01$", "", as.character(dates))
data_urls <- paste0(base_url, year_months, ".csv")

# FETCH AND STACK CSVS ----------------

# download, parse all files, write them to one csv
for (url in data_urls) {
     
     # download to temporary file
     tmpfile <- tempfile()
     download.file(url, destfile = tmpfile)
     
     # parse downloaded file, write to output csv, remove tempfile
     csv_parsed <- fread(tmpfile)
     fwrite(csv_parsed,
            file =  OUTPUT_PATH,
            append = TRUE)
     unlink(tmpfile)
     
}


```


# Data Storage and Databases


## (Big) Data Storage

 - $(I)$ How can we store large data sets permanently on a mass storage device in an efficient way (here, efficient can be understood as 'not taking up too much space')?
 - $(II)$ How can we load (parts of) this data set in an efficient way (here, efficient~fast) for analysis?

## We look at this problem in two situations: 

 - The data need to be stored locally (e.g., on the hard disk of our laptop).
 - The data can be stored on a server in the cloud.
 
## We look at three types of systems: 

 - Relational databases
 - Data warehouse solutions
 - Data lake/simple storage solutions

## Many new database types for Big Data

```{r whatis, echo=FALSE, out.width = "80%", fig.align='center', purl=FALSE, fig.cap="NoSQL/NewSQL systems. Source: https://img.deusm.com/informationweek/2014/06/1269559/NoSQL-&-NewSQL.jpg"}
include_graphics("https://img.deusm.com/informationweek/2014/06/1269559/NoSQL-&-NewSQL.jpg")
```




## Simple distinction

- *SQL/Relational Database Systems (RDBMS)*: Relational data model, tabular relations.
     - In use for a long time, very mature, very accurate/stable.
- *NoSQL ('non-SQL', sometimes 'Not only SQL')*: Different data models, column, document, key-value, graph.
     - Horizontal scaling.
     - Non-tabular data.
     - Typically used to handle very large amounts of data.

## Row-based vs Column-based

- *Row-based*: SQL databases (e.g., SQLite)
     - Changing one value, means updating a row.
     - Efficient when users often access many columns and rather few observations.
- *Column-based*: Some data warehouse and data lake systems, e.g., Google BigQuery
     - Efficient when users from time to time query few columns but vast amounts of observations.


## RDBMS basics

- *Relational data model*
     - Data split into several tables (avoid redundancies).
     - Tables are linked via key-variables/columns.
     - Save storage space.
- *Indexing*
     - Table columns (particularly keys) are indexed.
     - Reduces number of disk accesses required to query data.
     - Makes querying/loading of data more efficient/faster.




## Getting started with (R)SQLite

- [SQLite](https://sqlite.org/index.html)
     - Free, full-featured SQL database engine.
     - Widely used across platforms.
     - Typically pre-installed on Windows/MacOSX.
- [RSQLite](https://cran.r-project.org/web/packages/RSQLite/index.html)
     - Embeds SQLite in R.
     - Use SQLite from within an R session.


## Exercise 1:  First steps in SQLite (Terminal)

- Set up a new database called `mydb.sqlite`.

```{bash eval=FALSE}
cd materials/data 
```

```{bash eval= FALSE}
sqlite3 mydb.sqlite
```

```{sql eval = FALSE}
.tables
```


## Import data from CSV files

```{r echo=FALSE, message=FALSE}
library(DBI)
con <- dbConnect(RSQLite::SQLite(), "../data/mydb.sqlite")
```


```{sql connection=con, eval = FALSE}
CREATE TABLE econ(
"date" DATE,
"pce" REAL,
"pop" INTEGER,
"psavert" REAL,
"uempmed" REAL,
"unemploy" INTEGER
);

.mode csv
.import economics.csv econ
```


## Inspect the database


```{}
.tables
```

```{}
# econ
```

```{}
.schema econ
```

```{}
# CREATE TABLE econ(
# "date" DATE,
# "pce" REAL,
# "pop" INTEGER,
# "psavert" REAL,
# "uempmed" REAL,
# "unemploy" INTEGER
# );
```

## Set options for output

```{sql connection=con, eval = FALSE}
.header on
```

```{sql connection=con, eval = FALSE}
.mode columns
```



## Issue queries: Example 1

In our first query, we select all (`*`) variable values of the observation of January 1968.

```{sql connection=con}
select * from econ where date = '1968-01-01'
```

## Issue queries: Example 2

Now let's select all year/months in which there were more than 15 million unemployed, ordered by date.

```{sql connection=con}
select date from econ 
where unemploy > 15000
order by date;
```

## Close SQLite

When done working with the database, we can exit SQLite with the `.quit` command.




## Exercise 2: Indices and joins

 - Import several related tables.
 - Add indices to tables. 

## Initiate DB, import data

We set up a new database called `air.sqlite` and import the csv-file `flights.csv` (used in previous lectures) as a first table.

```{bash echo=TRUE, eval=FALSE}
# create database and run sqlite
sqlite3 air.sqlite

```

## Import data from CSVs


```{sql connection=con, eval = FALSE}
.mode csv
.import flights.csv flights
```


```{r echo=FALSE, message=FALSE}
library(DBI)
# set up a connection for the examples
con_air <- dbConnect(RSQLite::SQLite(), "../data/air_final.sqlite")
```


## Inspect the `flights` table

Again, we can check if everything worked out well with `.tables` and `.schema`.


```{sql connection=con_air, eval = FALSE}
.tables
.schema flights
```

## Related tables 

- [`airports.csv`](http://stat-computing.org/dataexpo/2009/airports.csv): Describes the locations of US Airports (relates to `origin` and `dest`).
- [`carriers.csv`](http://stat-computing.org/dataexpo/2009/carriers.csv): A listing of carrier codes with full names (relates to the `carrier`-column in `flights`.


```{r echo=FALSE, eval=FALSE}
# ASA source
URL_AIRPORTS <- "http://stat-computing.org/dataexpo/2009/airports.csv"
URL_CARRIERS <- "http://stat-computing.org/dataexpo/2009/carriers.csv"

# download
download.file(URL_AIRPORTS, destfile = "../data/airports.csv", quiet = TRUE)
download.file(URL_CARRIERS, destfile = "../data/carriers.csv", quiet = TRUE)

# re-format (facilitates import)
fwrite(fread("../data/airports.csv"), "../data/airports.csv")
fwrite(fread("../data/carriers.csv"), "../data/carriers.csv")

```


## Import related tables

Import from csv-file
```{sql connection=con_air, eval = FALSE}
.mode csv
.import airports.csv airports
.import carriers.csv carriers
```

Inspect the result
```{sql connection=con_air, eval = FALSE}
.tables
.schema airports
.schema carriers
```

## Issue queries with joins

 - Goal: A table containing flights data for all `United Air Lines Inc.`-flights departing from `Newark Intl` airport, ordered by flight number. 
 - For the sake of the exercise, we only show the first 10 results of this query (`LIMIT 10`).


## Issue queries with joins

```{sql connection=con_air, eval = TRUE}
SELECT 
year,
month, 
day,
dep_delay,
flight
FROM (flights INNER JOIN airports ON flights.origin=airports.iata) 
INNER JOIN carriers ON flights.carrier = carriers.Code
WHERE carriers.Description = 'United Air Lines Inc.'
AND airports.airport = 'Newark Intl'
ORDER BY flight
LIMIT 10;

```

## Add indices

```{sql connection=con_air, eval = FALSE}
CREATE INDEX iata_airports ON airports (iata);
CREATE INDEX origin_flights ON flights (origin);
CREATE INDEX carrier_flights ON flights (carrier);
CREATE INDEX code_carriers ON carriers (code);

```


## Re-run the query (with indices)

```{sql connection=con_air, eval = TRUE}
SELECT 
year,
month, 
day,
dep_delay,
flight
FROM (flights INNER JOIN airports ON flights.origin=airports.iata) 
INNER JOIN carriers ON flights.carrier = carriers.Code
WHERE carriers.Description = 'United Air Lines Inc.'
AND airports.airport = 'Newark Intl'
ORDER BY flight
LIMIT 10;

```



## SQLite from within R

- Use `RSQLite` to set up and query `air.sqlite` as shown above.
- All done from within an R session.


## Creating a new database with `RSQLite`

```{r}
# load packages
library(RSQLite)

# initiate the database
con_air <- dbConnect(SQLite(), "../data/air.sqlite")
```

## Importing data


```{r warning=FALSE, message=FALSE}
# load packages
library(data.table)

# import data into current R sesssion
flights <- fread("../data/flights.csv")
airports <- fread("../data/airports.csv")
carriers <- fread("../data/carriers.csv")

# add tables to database
dbWriteTable(con_air, "flights", flights)
dbWriteTable(con_air, "airports", airports)
dbWriteTable(con_air, "carriers", carriers)

```

## Issue queries with `RSQLite`

```{r}
# define query
delay_query <-
"SELECT 
year,
month, 
day,
dep_delay,
flight
FROM (flights INNER JOIN airports ON flights.origin=airports.iata) 
INNER JOIN carriers ON flights.carrier = carriers.Code
WHERE carriers.Description = 'United Air Lines Inc.'
AND airports.airport = 'Newark Intl'
ORDER BY flight
LIMIT 10;
"
```

## Issue queries with `RSQLite`

```{r}
# issue query
delays_df <- dbGetQuery(con_air, delay_query)
delays_df
```

## Close the connection to SQLite

```{r}
dbDisconnect(con_air)
```


```{r echo=FALSE}
# clean up
unlink("../data/air.sqlite")
```

# Data warehouses and BigQuery with R

## Data warehouse solutions in the cloud

- Unlike RMDBSs, main purpose is ususally analytics.
- Well organized/structured data, but not as stringent as RMDBS.
- Set up for large amounts of data.
- Flexible usage, cheap storage space, pay for data processing.
- Typically column-based.

## Google BigQuery

- Flexible, easy-to-use, hardly any set up costs.
- Pay per TB processed (only minor fees for storage etc.).
- `bigrquery`: R package to work with BigQuery via R (see https://bigrquery.r-dbi.org/)

## Get started with `bigrquery`

- Needed: Google Account.
- Go to https://cloud.google.com/bigquery.
- Click on "Try Big Query" (if new to this) or "Go to console" (if used previously).
- Create a project to use BigQuery with.
- In R: `install.packages("bigrquery")`


## A simple example: set up


```{r eval=FALSE}
# documents raw data processing

# load packages, credentials
library(data.table)
library(DBI)

# fix vars
BILLING <- "onlinemedia-slant" # the project ID on BigQuery (billing must be enabled)
PROJECT <- "bigquery-public-data" # the project name on BigQuery
DATASET <- "google_analytics_sample"

# connect to DB on BigQuery
con <- dbConnect(
     bigrquery::bigquery(),
     project = PROJECT,
     dataset = DATASET,
     billing = BILLING
)


```

## A simple example: run queries on BigQuery

```{r eval=FALSE}

# run query
#query <- "SELECT * FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`"
query <-
"
SELECT  totals.visits, totals.transactions, trafficSource.source, device.browser, device.isMobile, geoNetwork.city, geoNetwork.country, channelGrouping
  FROM
    `bigquery-public-data.google_analytics_sample.ga_sessions_*`
  WHERE
    _TABLE_SUFFIX BETWEEN '20160101'
    AND '20171231';
"

ga <- as.data.table(dbGetQuery(con, query, page_size=15000))
```


# Data lakes and simple storage service

## Data lakes

- Where all your data resides in the cloud (all kind of formats/structures).
- For a data analytics project: keep at least all the raw data here (or even all the data).
- Possible basis: simple storage service in the cloud (such as *AWS S3*)
- Then use Analytics tools to run on storage (e.g., AWS Athena)

## AWS S3 with R: first steps

Prerequisites:

- AWS account
- IAM credentials (keypair) with the right to access S3
- `install.packages("aws.s3")`

## Connect R-session with S3

- `AWS_ACCESS_KEY_ID`: your access key id (of the keypair with rights to use S3)
- `AWS_SECRET_KEY`: your access key (of the keypair with rights to use S3)
- `REGION`: the region in which your S3 bucket is located (e.g., `"eu-central-1"`)


```{r eval=FALSE}
# fetch current
Sys.setenv("AWS_ACCESS_KEY_ID" = AWS_ACCESS_KEY_ID,
           "AWS_SECRET_ACCESS_KEY" = AWS_SECRET_KEY,
           "AWS_DEFAULT_REGION" = REGION)
```

## Upload files to S3

- `BUCKET`: the name of the S3 bucket to upload data to.

```{r eval=FALSE}
# upload to bucket
put_object(
  file = "data/flights.csv", # the file you want to upload
  object = "flights.csv", # the name the file will have in the S3 bucket
  bucket = BUCKET
)
```





## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


